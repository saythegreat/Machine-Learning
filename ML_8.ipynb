{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6k0cmwsFuhXzmqJS74ZV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saythegreat/Machine-Learning/blob/main/ML_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_mh0d1wNb1o",
        "outputId": "7105bd95-e92b-4951-a66b-3671ebcff86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A5 AND: {'epochs': 130, 'final_sse': 0.0, 'converged': True, 'weights': [-0.05, 0.05, 0.05], 'sse_history': [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, '...']}\n",
            "A5 XOR: {'epochs': 1000, 'final_sse': 1.0, 'converged': False, 'weights': [-0.05, -0.05, -0.05], 'sse_history': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, '...']}\n",
            "A7 Customer pseudo-inverse: {'weights': [0.114, -0.0279, 0.0147, -0.0432, 0.0045], 'accuracy': 1.0}\n",
            "A8 SmallMLP AND: {'epochs': 1000, 'converged': False, 'sse_history': [0.5211, 0.5153, 0.5097, 0.5044, 0.4992, 0.4943, 0.4896, 0.4851, 0.4807, 0.4766, 0.4726, 0.4688, 0.4651, 0.4616, 0.4583]}\n",
            "A11 sklearn AND/XOR: {'AND_acc': 1.0, 'XOR_acc': 1.0}\n",
            "A12 sklearn Customer: {'accuracy': 1.0}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Lab08 - Neural Networks and Perceptrons\n",
        "Clean solution (A1â€“A12), structured by question.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.linalg import pinv\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ================= A1: Summation Unit =================\n",
        "def summation_unit(x, w):\n",
        "    return float(np.dot(w, x))\n",
        "\n",
        "# ================= A2: Activation Functions =================\n",
        "def step_activation(x): return np.where(x >= 0, 1, 0)\n",
        "def bipolar_step_activation(x): return np.where(x >= 0, 1, -1)\n",
        "def sigmoid_activation(x): return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "def tanh_activation(x): return np.tanh(x)\n",
        "def relu_activation(x): return np.maximum(0, x)\n",
        "def leaky_relu_activation(x, alpha=0.01): return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# ================= A3: Comparator / Error Unit =================\n",
        "def sse_error(y_true, y_pred):\n",
        "    return float(0.5 * np.sum((y_true - y_pred) ** 2))\n",
        "\n",
        "# ================= A4: Perceptron Training =================\n",
        "def perceptron_train(X, y, initial_w, lr=0.05, activation=\"step\",\n",
        "                     max_epochs=1000, convergence_sse=0.002):\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    n = X.shape[0]\n",
        "    Xb = np.hstack([np.ones((n, 1)), X])\n",
        "    w = initial_w.astype(float).copy()\n",
        "    sse_history = []\n",
        "    converged = False\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        net = Xb @ w\n",
        "        if activation == \"step\":\n",
        "            out = step_activation(net)\n",
        "            deltas = (y - out)\n",
        "            w += lr * (deltas @ Xb)\n",
        "        elif activation == \"bipolar\":\n",
        "            y_b = np.where(y == 0, -1, 1)\n",
        "            out = bipolar_step_activation(net)\n",
        "            deltas = (y_b - out)\n",
        "            w += lr * (deltas @ Xb)\n",
        "        elif activation == \"sigmoid\":\n",
        "            out = sigmoid_activation(net)\n",
        "            deltas = (y - out) * out * (1 - out)\n",
        "            w += lr * (deltas @ Xb)\n",
        "        elif activation == \"relu\":\n",
        "            out = step_activation(net)\n",
        "            deltas = (y - out)\n",
        "            w += lr * (deltas @ Xb)\n",
        "\n",
        "        sse = sse_error(y, out)\n",
        "        sse_history.append(round(sse, 4))\n",
        "        if sse <= convergence_sse:\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"epochs\": int(epoch),\n",
        "        \"final_sse\": round(float(sse), 4),\n",
        "        \"converged\": bool(converged),\n",
        "        \"weights\": [round(float(val), 4) for val in w],\n",
        "        \"sse_history\": sse_history[:15] + ([\"...\"] if len(sse_history) > 15 else [])\n",
        "    }\n",
        "\n",
        "# ================= A5: Datasets =================\n",
        "def and_dataset():\n",
        "    return np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1])\n",
        "def xor_dataset():\n",
        "    return np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,1,1,0])\n",
        "\n",
        "# ================= A6: Customer Dataset =================\n",
        "def customer_dataset():\n",
        "    data = [\n",
        "        (20,6,2,386,1),(16,3,6,289,1),(27,6,2,393,1),\n",
        "        (19,1,2,110,0),(24,4,2,280,1),(22,1,5,167,0),\n",
        "        (15,4,2,271,1),(18,4,2,274,1),(21,1,4,148,0),(16,2,4,198,0),\n",
        "    ]\n",
        "    df = pd.DataFrame(data, columns=[\"candies\",\"mangoes\",\"milk\",\"payment\",\"high\"])\n",
        "    X = df[[\"candies\",\"mangoes\",\"milk\",\"payment\"]].values.astype(float)\n",
        "    y = df[\"high\"].values.astype(int)\n",
        "    return X, y\n",
        "\n",
        "# ================= A7: Pseudo-Inverse =================\n",
        "def pseudo_inverse_solution(X, y):\n",
        "    n = X.shape[0]\n",
        "    Xb = np.hstack([np.ones((n,1)), X])\n",
        "    w = pinv(Xb) @ y\n",
        "    raw = Xb @ w\n",
        "    preds = (raw >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"weights\": [round(float(val), 4) for val in w],\n",
        "        \"accuracy\": float((preds == y).mean())\n",
        "    }\n",
        "\n",
        "# ================= A8: Backprop MLP for AND =================\n",
        "class SmallMLP:\n",
        "    def __init__(self, n_inputs, n_hidden=2, lr=0.05, max_epochs=1000, conv_sse=0.002):\n",
        "        rng = np.random.RandomState(0)\n",
        "        self.W1 = rng.randn(n_hidden, n_inputs+1) * 0.1\n",
        "        self.W2 = rng.randn(1, n_hidden+1) * 0.1\n",
        "        self.lr, self.max_epochs, self.conv_sse = lr, max_epochs, conv_sse\n",
        "\n",
        "    def sigmoid(self,x): return 1/(1+np.exp(-np.clip(x,-500,500)))\n",
        "    def sigmoid_deriv(self,y): return y*(1-y)\n",
        "\n",
        "    def forward(self,X):\n",
        "        n = X.shape[0]\n",
        "        Xb = np.hstack([np.ones((n,1)),X])\n",
        "        z1 = self.sigmoid(Xb @ self.W1.T)\n",
        "        z1b = np.hstack([np.ones((n,1)),z1])\n",
        "        out = self.sigmoid(z1b @ self.W2.T).ravel()\n",
        "        return out,Xb,z1,z1b\n",
        "\n",
        "    def train(self,X,y):\n",
        "        sse_history=[]\n",
        "        for epoch in range(1,self.max_epochs+1):\n",
        "            out,Xb,z1,z1b = self.forward(X)\n",
        "            err = y-out\n",
        "            sse=0.5*np.sum(err**2)\n",
        "            sse_history.append(round(float(sse),4))\n",
        "            if sse<=self.conv_sse:\n",
        "                return {\"epochs\":epoch,\"converged\":True,\"sse_history\":sse_history[:15]}\n",
        "            delta_out = err*self.sigmoid_deriv(out)\n",
        "            grad_W2 = delta_out.reshape(-1,1).T @ z1b\n",
        "            delta_hidden = (delta_out.reshape(-1,1) @ self.W2[:,1:]) * self.sigmoid_deriv(z1)\n",
        "            grad_W1 = delta_hidden.T @ Xb\n",
        "            self.W2 += self.lr*grad_W2\n",
        "            self.W1 += self.lr*grad_W1\n",
        "        return {\"epochs\":self.max_epochs,\"converged\":False,\"sse_history\":sse_history[:15]}\n",
        "\n",
        "# ================= A11 & A12: sklearn MLP =================\n",
        "def sklearn_mlp_and_xor():\n",
        "    X_and,y_and = and_dataset()\n",
        "    X_xor,y_xor = xor_dataset()\n",
        "    clf_and = MLPClassifier(hidden_layer_sizes=(),activation=\"logistic\",solver=\"lbfgs\",max_iter=1000)\n",
        "    clf_and.fit(X_and,y_and)\n",
        "    clf_xor = MLPClassifier(hidden_layer_sizes=(2,),activation=\"logistic\",solver=\"lbfgs\",max_iter=10000)\n",
        "    clf_xor.fit(X_xor,y_xor)\n",
        "    return {\n",
        "        \"AND_acc\": float(clf_and.score(X_and,y_and)),\n",
        "        \"XOR_acc\": float(clf_xor.score(X_xor,y_xor))\n",
        "    }\n",
        "\n",
        "def sklearn_mlp_customer():\n",
        "    X,y = customer_dataset()\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(5,),activation=\"logistic\",max_iter=2000,random_state=0)\n",
        "    clf.fit(Xs,y)\n",
        "    preds = clf.predict(Xs)\n",
        "    return {\"accuracy\": float((preds==y).mean())}\n",
        "\n",
        "# ================= MAIN =================\n",
        "if __name__ == \"__main__\":\n",
        "    init_w = np.array([10,0.2,-0.75])\n",
        "    X_and,y_and = and_dataset()\n",
        "    X_xor,y_xor = xor_dataset()\n",
        "\n",
        "    print(\"A5 AND:\", perceptron_train(X_and, y_and, init_w, activation=\"step\"))\n",
        "    print(\"A5 XOR:\", perceptron_train(X_xor, y_xor, init_w, activation=\"step\"))\n",
        "    print(\"A7 Customer pseudo-inverse:\", pseudo_inverse_solution(*customer_dataset()))\n",
        "    mlp = SmallMLP(2)\n",
        "    print(\"A8 SmallMLP AND:\", mlp.train(X_and,y_and))\n",
        "    print(\"A11 sklearn AND/XOR:\", sklearn_mlp_and_xor())\n",
        "    print(\"A12 sklearn Customer:\", sklearn_mlp_customer())\n"
      ]
    }
  ]
}